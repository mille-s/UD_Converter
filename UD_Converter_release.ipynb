{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/UD_Converter/blob/main/UD_Converter_release.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to download and unzip the working folder (once run, click \"Refresh\" on the top left corner to see the folders)\n",
        "from IPython.display import clear_output \n",
        "! gdown 1m4KZt3HO1O0bgLiddBfpDkpvMjS6XSbZ\n",
        "! unzip /content/UD_Converter_colab.zip\n",
        "clear_output()\n",
        "\n",
        "# If any issue, the zip file can be found in the following shared folder: https://drive.google.com/file/d/1m4KZt3HO1O0bgLiddBfpDkpvMjS6XSbZ/view?usp=sharing"
      ],
      "metadata": {
        "id": "gW1f_juqCmNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ocgpC5nfP8h"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "# authors: simon mille, alex shvets\n",
        "\n",
        "import os, shutil\n",
        "from shutil import copyfile\n",
        "import sys\n",
        "import glob\n",
        "import subprocess\n",
        "from subprocess import Popen, PIPE\n",
        "import timeit\n",
        "import datetime\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "start = timeit.default_timer()\n",
        "\n",
        "#============================================================================================================\n",
        "# GENERAL PARAMETERS (please read comments before parameters to avoid most errors)\n",
        "#============================================================================================================\n",
        "# path to working folder (which must contain the buddy-core tools that convert to t2)\n",
        "path_jars = '/content/UD_Converter'\n",
        "# !!! path to input folder; file names in the input folder should not contain spaces or parentheses\n",
        "# !!! needs to exist and have some CoNLL(-U) files inside, if possible with a 2-letter prefix to indicate the language (e.g. en_ewt-UD.conllu, fr_myfile.conllu); supported: en, fr, es\n",
        "inputFolder = '/content/UD_Converter/inputs'\n",
        "# path to output folder; will be created if does not exist\n",
        "outputFolder = '/content/UD_Converter/out'\n",
        "\n",
        "# !!! file extension: input file should be in '.conllu' format, with or without metadata (lines starting with '#'); 10- or 14-column '.conll' format is also accepted\n",
        "inputFormat = 'conllu'\n",
        "# number of structures per file: big files need to be cut into smaller files containing this amount of sentences (10,000 max recommended); separated files are brought back together at the end of the conversion.\n",
        "strPerFile = '10000'\n",
        "# perform structure well-formedness and file alignment checks and create debug files ('yes'/'no')\n",
        "debug = 'yes'\n",
        "\n",
        "#============================================================================================================\n",
        "# CONVERSION PARAMETERS ('yes'/'no')\n",
        "# Specifications of the T1 and T2 structures and links to papers can be found on the SRST page: http://taln.upf.edu/pages/msr2020-ws/SRST.html\n",
        "#============================================================================================================\n",
        "# generate input strutures for surface ('t1') or deep ('t2') tracks\n",
        "track = 't2'\n",
        "# keep deep structures from previous executions (for SRST data, we need 2 executions, first for deep and then for surf files): 'yes' or whatever\n",
        "keep_deep = 'no'\n",
        "# scramble files or not, i.e. change order of words ('yes', 'no'; SRST: 'yes')\n",
        "scramble = 'yes'\n",
        "# keep relative ordering of punctuation marks (SRST surface: 'yes'; deep: 'no')\n",
        "orderPunc = 'yes'\n",
        "if track == 't2':\n",
        "  orderPunc = 'no'\n",
        "# keep relative ordering of cunjuncts in coordination (SRST: 'yes')\n",
        "orderConj = 'yes'\n",
        "# keep relative ordering of MWE components (SRST: 'yes')\n",
        "orderMWE = 'yes'\n",
        "# data type: '1' is for train, '2' is for test\n",
        "dt = '1'\n",
        "\n",
        "#============================================================================================================\n",
        "# ADDITIONAL PARAMETERS FOR DEEP STRUCTURES\n",
        "#============================================================================================================\n",
        "default = 'no'\n",
        "# keep ID of position of each word from the original conllu file (column #1) in the deep structure (SRST: 'yes' if scrambled); note that original IDs are not kept in test files\n",
        "originalID = 'yes'\n",
        "# reduce deep tree to the minimal subtree that contains both object=true and subject=true nodes as indicated in the FEATS column of the input file ('yes' or whatever; SRST: 'no')\n",
        "reduce_tree = 'yes'\n",
        "# keep form from the original conllu file (column #2) in the deep structure? (SRST: 'no')\n",
        "originalForm = default\n",
        "# keep xpos from the original conllu file (column #5) in the deep structure (SRST: 'no')\n",
        "originalXpos = default\n",
        "# keep parentheses in the deep structure (SRST: 'no')\n",
        "parentheses = default\n",
        "# keep quotation marks in the deep structure (SRST: 'no')\n",
        "quotationMarks = default\n",
        "\n",
        "# path to temporary folders\n",
        "tmpIn = os.path.join(path_jars, 'tmpIn')\n",
        "tmpOut = os.path.join(path_jars, 'tmpOut')\n",
        "# path to the folder in which the debug info is stored\n",
        "debugFolder = os.path.join(outputFolder, 'debug')\n",
        "# Define output subfolders\n",
        "deepOut = os.path.join(outputFolder, 'deep')\n",
        "surfOut = os.path.join(outputFolder, 'surf')\n",
        "sentOut = os.path.join(outputFolder, 'sent')\n",
        "\n",
        "# Clear the debug folder before starting the conversion\n",
        "try:\n",
        "  shutil.rmtree(debugFolder)\n",
        "except Exception as e:\n",
        "  pass\n",
        "\n",
        "# Choose which folder(s) to keep from previous generations\n",
        "if keep_deep == 'yes':\n",
        "  try:\n",
        "    shutil.rmtree(surfOut)\n",
        "  except Exception as e:\n",
        "    pass\n",
        "    \n",
        "  try:\n",
        "    shutil.rmtree(sentOut)\n",
        "  except Exception as e:\n",
        "    pass\n",
        "\n",
        "else:\n",
        "  try:\n",
        "    shutil.rmtree(outputFolder)\n",
        "  except Exception as e:\n",
        "    pass\n",
        "\n",
        "# In case we did not delete them after using them below\n",
        "try:\n",
        "  shutil.rmtree(tmpIn)\n",
        "except Exception as e:\n",
        "  pass\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(tmpOut)\n",
        "except Exception as e:\n",
        "  pass\n",
        "\n",
        "# Create final and temp output folders\n",
        "if not os.path.exists(outputFolder):\n",
        "  os.makedirs(outputFolder)\n",
        "\n",
        "if track == 't2':\n",
        "  if not os.path.exists(deepOut):\n",
        "    os.makedirs(deepOut)\n",
        "if not os.path.exists(surfOut):\n",
        "  os.makedirs(surfOut)\n",
        "if not os.path.exists(sentOut):\n",
        "  os.makedirs(sentOut)\n",
        "\n",
        "deepOutTmp = os.path.join(tmpOut, 'deep')\n",
        "if track == 't2':\n",
        "  if not os.path.exists(deepOutTmp):\n",
        "    os.makedirs(deepOutTmp)\n",
        "surfOutTmp = os.path.join(tmpOut, 'surf')\n",
        "if not os.path.exists(surfOutTmp):\n",
        "  os.makedirs(surfOutTmp)\n",
        "sentOutTmp = os.path.join(tmpOut, 'sent')\n",
        "if not os.path.exists(sentOutTmp):\n",
        "  os.makedirs(sentOutTmp)\n",
        "\n",
        "os.makedirs(debugFolder)\n",
        "\n",
        "print('\\n==============================\\nPre-processing input file(s)...\\n==============================')\n",
        "# File splitting: the converter cannot process files that are too big, so they need to be split. For regular-sized sentences, 10,000 sentences per file should do.\n",
        "# Parameters:\n",
        "# [1] path to input folder\n",
        "# [2] encoding of input files\n",
        "# [3] number of structures per file\n",
        "# [4] split once ('first'), or every time the threshold in [3] is reached ('all')\n",
        "# [5] path to temp folder used to store split files\n",
        "# This part creates a tmpIn folder in which all split files are stored\n",
        "print('\\nChecking files to split...\\n')\n",
        "path_splitFiles = os.path.join(path_jars, 'splitFiles.py')\n",
        "!python {path_splitFiles} {inputFolder} 'utf-8' {strPerFile} 'all' {tmpIn}\n",
        "\n",
        "# Conversion to format that can be loaded by the .jar.\n",
        "# Parameters:\n",
        "# [1] extension of input files (inputFormat)\n",
        "# [2] path to temp folder used to store split files\n",
        "# This part creates a folder within the tmpIn folder, which contains files in the CoNLL'09 format with all the information needed for the conversion.\n",
        "print('\\nConverting file format...\\n')\n",
        "if dt == '2':\n",
        "  originalID = 'no'\n",
        "convertFolder = tmpIn\n",
        "path_conllu2conll = os.path.join(path_jars, 'conllu2conll.py')\n",
        "!python {path_conllu2conll} {inputFormat} {convertFolder} {originalID} {originalForm} {originalXpos} {parentheses} {quotationMarks} {orderPunc} {orderConj} {orderMWE} {track} {dt} {sentOutTmp} {reduce_tree}\n",
        "\n",
        "# Scrambling of the files to remove order information.\n",
        "# Parameters:\n",
        "# [1]\n",
        "# [2]\n",
        "\n",
        "print('\\n==============================\\nScrambling input file(s)...\\n==============================')\n",
        "# Create output folder if does not exist\n",
        "# This part takes the enriched CoNLL'09 files and scrambles each file (so that the original order is not explicit anymore). The resulting structures are kept in a folder within tmpIn, with the -scrambled extension.\n",
        "\n",
        "files2Scramble = [f for f in os.listdir(os.path.join(tmpIn, 'conllu2conll')) if '.conll' in f]\n",
        "path_conllScramble = os.path.join(path_jars, 'conllScramble.py')\n",
        "  \n",
        "for file2Scramble in files2Scramble:\n",
        "  if scramble == 'yes':\n",
        "    !python {path_conllScramble} {file2Scramble} {surfOutTmp} {track} {dt} {tmpIn}\n",
        "  else:\n",
        "    print('\\nNo file scrambled!')\n",
        "    copyfile(os.path.join(tmpIn, 'conllu2conll', file2Scramble), os.path.join(surfOutTmp, file2Scramble))\n",
        "\n",
        "if track == 't2':\n",
        "  print('\\n==============================\\nStarting with conversion...\\n==============================')\n",
        "  # Conversion of UD files into Deep representation. A log file is created in the debug folder (log.txt), in which the word 'Error' is printed in case a structure could not be processed. For processing big files, 1g of memory may be needed.\n",
        "  # This part performs the conversion to deep structures of the scrambled enriched CoNLL'09 structures stored in tmpIn. The results are saved in the tmpOut folder, each file in a folder named as the input file.\n",
        "  # Parameters:\n",
        "  # [1] path to input folder\n",
        "  # [2] -o path to temporary output folder\n",
        "  list_prefixes = ['en', 'es', 'fr']\n",
        "  files = [f for f in os.listdir(os.path.join(surfOutTmp)) if '.conll' in f]\n",
        "  for f in files:\n",
        "    filepath = os.path.join(surfOutTmp,f)\n",
        "    prefix = f.split('_', 1)[0]\n",
        "    if prefix not in list_prefixes:\n",
        "      print('\\nRunning default converter (en)...\\n---------------\\n')\n",
        "      path_buddy_core = os.path.join(path_jars, 'buddy-core-0.1.1-en.jar')\n",
        "      with open(os.path.join(debugFolder, 'log_deep_processing.txt'), 'a') as logfile:\n",
        "        proc = subprocess.Popen(['java', '-Xmx1g', '-jar', path_buddy_core, os.path.join(surfOutTmp,f), '-o', deepOutTmp], stdout = subprocess.PIPE, universal_newlines=True)\n",
        "        for line in proc.stdout:\n",
        "          sys.stdout.write(line)\n",
        "          logfile.write(line)\n",
        "    else:\n",
        "      print('\\nRunning converter according to prefix of input file ('+prefix+')...\\n---------------\\n')\n",
        "      path_buddy_core = os.path.join(path_jars, 'buddy-core-0.1.1-'+prefix+'.jar')\n",
        "      with open(os.path.join(debugFolder, 'log_deep_processing.txt'), 'a') as logfile:\n",
        "        proc = subprocess.Popen(['java', '-Xmx1g', '-jar', path_buddy_core, os.path.join(surfOutTmp,f), '-o', deepOutTmp], stdout = subprocess.PIPE, universal_newlines=True)\n",
        "        for line in proc.stdout:\n",
        "          sys.stdout.write(line)\n",
        "          logfile.write(line)\n",
        "      \n",
        "try:\n",
        "  shutil.rmtree(tmpIn)\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "print('\\n==============================\\nConcatenating output files...\\n==============================')\n",
        "# File concatenation: the big files that had been split in smaller files are brought back together.\n",
        "# Parameters:\n",
        "# [1] path to input folder\n",
        "# [2] path to output folder\n",
        "# [3] encoding of input files\n",
        "# [4] encoding of output files\n",
        "# [5] extension of output files (in this case the same as the input format, 'conllu')\n",
        "# [6] the type of structure that have to be brought together (deep, surf(ace), sent(ences))\n",
        "path_concatenateFiles = os.path.join(path_jars, 'concatenateFiles.py')\n",
        "if os.path.exists(deepOutTmp):\n",
        "  dir_contents_deep = [x for x in os.listdir(deepOutTmp) if not x.startswith('.')]\n",
        "  if len(dir_contents_deep) > 0:\n",
        "    print('\\nConcatenating deep structures...')\n",
        "    !python {path_concatenateFiles} {deepOutTmp} {deepOut} 'utf-8' 'utf-8' {inputFormat} 'deep' {track} {dt}\n",
        "\n",
        "if os.path.exists(surfOutTmp):\n",
        "  dir_contents_surf = [x for x in os.listdir(surfOutTmp) if not x.startswith('.')]\n",
        "  if len(dir_contents_surf) > 0:\n",
        "    print('\\nConcatenating surface structures...')\n",
        "    !python {path_concatenateFiles} {surfOutTmp} {surfOut} 'utf-8' 'utf-8' {inputFormat} 'surf' {track} {dt}\n",
        "  \n",
        "if os.path.exists(sentOutTmp):\n",
        "  dir_contents_sent = [x for x in os.listdir(sentOutTmp) if not x.startswith('.')]\n",
        "  if len(dir_contents_sent) > 0:\n",
        "    print('\\nConcatenating sentences...')\n",
        "    !python {path_concatenateFiles} {sentOutTmp} {sentOut} 'utf-8' 'utf-8' 'txt' 'sent' {track} {dt}\n",
        "\n",
        "try:\n",
        "  shutil.rmtree(tmpOut)\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "\n",
        "if debug == 'yes':\n",
        "  print('\\n==============================\\nChecking outputs...\\n==============================')\n",
        "\n",
        "  try:\n",
        "    os.remove(os.path.join(debugFolder, 'log_alignments.txt'))\n",
        "  except Exception as e:\n",
        "    pass\n",
        "    \n",
        "  print('\\nChecking alignments between original UD and surface files......\\n')\n",
        "  path_checkAlignments = os.path.join(path_jars, 'checkAlignments.py')\n",
        "  path_checkWellFormedness = os.path.join(path_jars, 'checkWellFormedness.py')\n",
        "\n",
        "  !python {path_checkAlignments} {inputFolder} {surfOut} {debugFolder} 'utf-8' 'UD2surf' {dt} {scramble}\n",
        "\n",
        "  # If the deep structures were kept from a previous execution, check their alignment too\n",
        "  if keep_deep == 'yes':\n",
        "    track = 't2'\n",
        "\n",
        "  if track == 't2':\n",
        "\n",
        "    print('\\nChecking alignments between surface and deep files......\\n')\n",
        "    !python {path_checkAlignments} {surfOut} {deepOut} {debugFolder} 'utf-8' 'surf2deep' {dt} {scramble}\n",
        "\n",
        "    print('\\nChecking alignments between original UD and deep files......\\n')\n",
        "    !python {path_checkAlignments} {inputFolder} {deepOut} {debugFolder} 'utf-8' 'UD2deep' {dt} {scramble}\n",
        "    \n",
        "    print('\\nChecking deep tree well-formedness...\\n')\n",
        "    # File check: a small script that checks the contents of the output files. It looks for configurations that in theory should not happen: disconnections, cycles, repeated argument numbers, multiple incoming dependencies (in case of tree input). A log file is created in the debug folder (log_treeness.txt), and optionally, folders with the ill-formed files.\n",
        "    # Parameters:\n",
        "    # [1] path to output debug folder\n",
        "    # [2] path to file to be checked\n",
        "    # [3] encoding of input files\n",
        "    # [4] type of structure to be checked ('tree' or 'graph')\n",
        "    # [5] OPTIONAL: path to original files from which the whole conversion started ('inputFolder'). If used, a folder with files that are ill-formed will be created.\n",
        "    # [6] ONLY IF [5]: input format of original files (inputFormat)\n",
        "    listFinalFilepaths = glob.glob(os.path.join(deepOut, '*.'+inputFormat))\n",
        "    for outFile in listFinalFilepaths:\n",
        "      print(outFile)\n",
        "      !python {path_checkWellFormedness} {debugFolder} {outFile} 'utf-8' 'tree' {inputFolder} {inputFormat}\n",
        "  else:\n",
        "    pass\n",
        "  \n",
        "stop = timeit.default_timer()\n",
        "timeConversion = str(datetime.timedelta(seconds=round((stop - start), 2)))\n",
        "print('\\n--------------------\\nDONE')\n",
        "print(timeConversion)\n",
        "print('--------------------\\n')\n",
        "\n",
        "foTime = codecs.open(os.path.join(debugFolder, 'log_time.txt'),'w','utf-8')\n",
        "foTime.write(timeConversion)\n",
        "foTime.close()"
      ]
    }
  ]
}